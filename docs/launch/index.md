---
sidebar_position: 1
---

# Introduction

- questions?
 Need a unified ML job scheduler for fragmented ml pipelines and diverse gen-ai applications?

- Our solution
diagram with action by action:

1. define your ML job without code change in a declarative format (e.g., YAML) or reuse our pre-built job templates -> 
2. launch your ML job with just one-line CLI or one-click in GUI
3. search cheaper resource across a large number of GPU providers without price lock-in
4. provision automatically for your GPU resources and the software environment setup tailored for your job 
5. manage concurrent jobs in the provisioned GPU cluster with job queue support 
5. orchestrate your ml job across multi-nodes/geo-distributed environment, it can be model deployment across GPU nodes, distributed training, or even federated learning across-clouds. 
6. run your job with rich observability features so you can monitor the real-time billing, metrics, logs, system performances, as well as diagnose performance bottleneck by fine-grained profiling. 

- features

1. Find the lower prices without cloud vendor lock-in, in any clouds

2. The highest GPU availability, Provision in all zones/regions/clouds, even individual GPU contributors from the community

3. User-friendly Ops to save time on environment management (AI docker hub for developers)

4. 

5. On-premises GPU cluster management

6. 

7. Scheduling strategies to save money or request resources in a higher priority
-




features:
- GPU availability: Find the lower prices without cloud vendor lock-in, in any clouds
- Scheduling strategies to save money or request resources in a higher priority

- Cluster management: cloud cluster or on-prem cluster, up to your choice; bind devices and build them as a cluster the simple way; run jobs on cluster with one-line CLI; maintain job Queue and job priority; collaborate with other team members
- Strong reliability via handling failover, fault-tolerant, preemption, etc.

# CLIs

fedml login -h
